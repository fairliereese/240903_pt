{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de71228-a45b-404e-a23f-ae5a010dab34",
   "metadata": {},
   "source": [
    "## Goal: get SNPs both CLOSE to SJs (exonic, +- 10 bp) and within SSs themselves\n",
    "\n",
    "* 1. Use these to compute what % of novel SJs only in hap1/hap2 have a +- 10bp SNP OR SNP in the SS (should be very close to another I'm computing) AND\n",
    "* 2. Subset on just known SJs to compute a known SJ-proximal SNP density score that I can use to correlate against the % increase in ICs / SJs detected to predict how much novel splicing we're missing in populations / samples that we actually didn't profile here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe2a6747-ed87-420f-b26e-933e1bdb456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import swan_vis as swan\n",
    "import yaml\n",
    "from snakemake.io import expand\n",
    "import cerberus\n",
    "import pyranges as pr\n",
    "import upsetplot\n",
    "\n",
    "p = os.path.dirname(os.getcwd())\n",
    "sys.path.append(p)\n",
    "\n",
    "from scripts.utils import *\n",
    "from scripts.vcf_utils import *\n",
    "from scripts.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "706a3d5a-501a-4e7b-8127-776200a50b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "od = '../'\n",
    "\n",
    "def proc_cfg(entry, od):\n",
    "    entry = entry.replace('../../', '')\n",
    "    entry = od+entry\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4803fe2b-03a2-4ea2-96cb-fdeeb4474135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_meta()\n",
    "df = df.loc[~df['sample'].str.contains('_')]\n",
    "df['lab_sample'] = df['lab_number_sample'].astype(str)+'_'+\\\n",
    "                      df['lab_sampleid'].astype(str)+'_'+\\\n",
    "                      df['cell_line_id'].astype(str)\n",
    "df.columns\n",
    "df = df[['cell_line_id', 'sample', 'hapmap_DNA_ID',\n",
    "          'map_reads_assemblymap','lab_sample', 'population']].drop_duplicates()\n",
    "\n",
    "temp_df = pd.read_csv('../snakemake/transcript_discovery_personal/cell_line_ids.txt', header=None, names=['cell_line_id'])\n",
    "\n",
    "# make a 1000g cell line id col\n",
    "df['cell_line_id_1000g'] = df.cell_line_id\n",
    "\n",
    "inds = df.loc[~df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())].index\n",
    "df.loc[inds, 'cell_line_id_1000g'] = df.loc[inds, 'hapmap_DNA_ID']\n",
    "len(df.index)\n",
    "\n",
    "# limit to just those in 1000g\n",
    "df = df.loc[df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())]\n",
    "assert len(df.index) == 30\n",
    "\n",
    "# TODO bad sample that hasn't finished on espresso\n",
    "# bad_samples = ['NA19328']\n",
    "# df = df.loc[~df.cell_line_id_1000g.isin(bad_samples)]\n",
    "\n",
    "hap = ['hap1', 'hap2']\n",
    "\n",
    "meta_df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162da471-a4d7-4c02-822c-802fda4696c5",
   "metadata": {},
   "source": [
    "## We want to know what % of 1.5 NOVEL sjs that we 1. discover only in personal haplotypes that have snps +- 10 of splice junctions OR in splice sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3265038b-9d8f-4167-860b-3c7ee71a985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = proc_cfg(config['lr']['td_personal']['sqanti']['sj_summary'],od)\n",
    "df = pd.read_csv(file)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df['detected'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42605873-a0f0-46db-84d6-0c58a911efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 only novel sjs\n",
    "temp = df.loc[df.junction_novelty=='novel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c453b-b5fe-45c5-8bbc-ca02d04c8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the detection from mapping\n",
    "temp = temp[['sj_id', 'cell_line_id', 'map_genome', 'detected']]\n",
    "temp = temp.drop_duplicates()\n",
    "temp = temp.pivot(index=['sj_id', 'cell_line_id'], \n",
    "                columns=['map_genome'],\n",
    "                values=['detected'])\n",
    "\n",
    "# flatten\n",
    "temp.columns = temp.columns.get_level_values(1)\n",
    "temp.columns.name = None\n",
    "\n",
    "# reset index to make it a flat DataFrame\n",
    "temp = temp.reset_index()\n",
    "\n",
    "# fill missing values with False\n",
    "temp = temp.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d8392-d537-474f-bbbe-80bf5711c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get those that are detected uniquely in hap1/2 \n",
    "# (ie those not in hg38)\n",
    "temp = temp.loc[(temp.hg38==False)&\n",
    "                ((temp.hap1+temp.hap2)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db5ca0-c19e-439f-841d-3c617f48ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. process snp content in each sj\n",
    "sj_snp_df = pd.DataFrame()\n",
    "for c in meta_df['cell_line_id_1000g'].tolist():\n",
    "    file = proc_cfg(expand(config['lr']['td_personal']['exon_vars']['sj_12nt_vcf_intersect'],\n",
    "                cell_line_id=c)[0],\n",
    "                od)\n",
    "    df = pr.read_bed(file).df\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # just keep uniq. sj_ids\n",
    "    df.rename({'ThickStart': 'sj_id'}, axis=1, inplace=True)\n",
    "    df = df[['sj_id']]\n",
    "    \n",
    "    df['cell_line_id'] = c\n",
    "    df['exon_12nt_has_var'] = True\n",
    "    sj_snp_df = pd.concat([sj_snp_df, df], axis=0)\n",
    "sj_snp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a4740-22e1-4533-972f-811537048c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. process snp content in each sj\n",
    "sj_snp_df = pd.DataFrame()\n",
    "for c in meta_df['cell_line_id_1000g'].tolist():\n",
    "    file = proc_cfg(expand(config['lr']['td_personal']['exon_vars']['sj_10nt_vcf_intersect'],\n",
    "                cell_line_id=c)[0],\n",
    "                od)\n",
    "    df = pr.read_bed(file).df\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # just keep uniq. sj_ids\n",
    "    df.rename({'ThickStart': 'sj_id'}, axis=1, inplace=True)\n",
    "    df = df[['sj_id']]\n",
    "    \n",
    "    df['cell_line_id'] = c\n",
    "    df['exon_12nt_has_var'] = True\n",
    "    sj_snp_df = pd.concat([sj_snp_df, df], axis=0)\n",
    "sj_snp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11653957-755d-4c49-9622-61fb93391855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the # and % of hap1/hap2-spec sjs (per sample) that hav variants in +-10bp + SSs\n",
    "temp = temp[['sj_id', 'cell_line_id', 'exon_12nt_has_var']]\n",
    "temp = temp.groupby(['cell_line_id', 'exon_12nt_has_var']).nunique().reset_index().rename({'sj_id':'n_sj'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbc77-088e-4756-b182-36a616f6ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add total & compute %\n",
    "temp['total'] = temp.groupby('cell_line_id')['n_sj'].transform('sum')\n",
    "temp['perc'] = (temp.n_sj/temp.total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b43668-bc52-4669-ba18-c72122e86049",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_plot_settings(aspect='square')\n",
    "temp = temp.loc[temp.has_explanation==True]\n",
    "ax  = sns.boxplot(temp, \n",
    "                  y='perc', showfliers=False)\n",
    "sns.stripplot(temp,\n",
    "              y='perc',\n",
    "              size=10)\n",
    "ax.set(ylabel='% of novel, hg38 undet. SJs\\nwhich have a variant either\\nin SS or exonic +-10bp')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8f6c6-1a0f-4df2-9238-c679dcc15df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076562c-400e-4c78-8d02-b4ec1bf21312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f9b5d-12ac-42dd-9e56-c0fe6887a6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c5090-c777-493b-88d7-d672194d5a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e3be258-20fa-4bb8-82e1-1c901a3250c0",
   "metadata": {},
   "source": [
    "## Dev for getting the +- 10 + SS bed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14fc5a81-1087-4a42-bc95-6fe9658cba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = proc_cfg(config['lr']['td_personal']['sqanti']['sj_summary'],od)\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7158cd-643b-4fa1-90cb-69e5451d1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24934518\n",
      "8311506\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bbc28-d15a-4c1c-adce-01e5070e5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_back = temp.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e51fbd50-7b27-4910-97af-7b1c722646f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = temp_back.copy(deep=True)\n",
    "# check w/ just 1 genome + cell line at first so I can \n",
    "# dl GTF and check it\n",
    "temp = temp.loc[(temp.cell_line_id=='HG03732')&\\\n",
    "                (temp.hap1==True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a884cda5-e197-4b65-8f79-8526c39ba02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# remove sqanti genome and drop dupes\n",
    "# the sqanti genome / sqanti metrics SHOULD be irrelevant here\n",
    "df = df.drop(['sqanti_genome', 'canonical', 'splice_motif'], axis=1)\n",
    "print(len(df.index))\n",
    "df = df.drop_duplicates()\n",
    "print(len(df.index))\n",
    "\n",
    "# then make sure that there are no dupe. sj+sj nov+sample+map genome\n",
    "temp = df.loc[df[['sj_id', 'junction_novelty',\n",
    "                  'cell_line_id', 'map_genome', 'start_site_novelty',\n",
    "                  'end_site_category']].duplicated(keep=False)]\n",
    "assert len(temp.index) == 0\n",
    "del temp\n",
    "\n",
    "df.rename({'end_site_category': 'end_site_novelty'}, axis=1, inplace=True)\n",
    "\n",
    "# transform to be t/f for each ic per genome\n",
    "temp = pd.crosstab(index=[df.sj_id, df.junction_novelty,\n",
    "                                 df.start_site_novelty,\n",
    "                                 df.end_site_novelty, df.cell_line_id],\n",
    "                          columns=df.map_genome,\n",
    "                          values=df.map_genome,\n",
    "                          aggfunc=lambda x: True).fillna(False).reset_index()\n",
    "\n",
    "\n",
    "temp[['Chromosome', 'Strand', 'Start', 'End']] = temp.sj_id.str.split('_', expand=True)\n",
    "temp = temp[['Chromosome', 'Strand', 'Start', 'End',\n",
    "             'cell_line_id', 'sj_id']].drop_duplicates()\n",
    "\n",
    "temp.Start = temp.Start.astype(int)\n",
    "temp.End = temp.End.astype(int)\n",
    "assert len(temp.loc[temp.Start>temp.End])==0\n",
    "\n",
    "# melt to 5' and 3'\n",
    "temp = temp.melt(id_vars=['Chromosome', 'Strand', 'cell_line_id', 'sj_id'],\n",
    "                 value_vars=['Start', 'End'])\n",
    "temp['sj_loc'] = ''\n",
    "temp.loc[temp.map_genome=='Start', 'sj_loc'] = 'start'\n",
    "temp.loc[temp.map_genome=='End', 'sj_loc'] = 'end'\n",
    "\n",
    "temp.rename({'value':'Start'}, axis=1, inplace=True)\n",
    "# need to verify that this is working using like one motif or something make\n",
    "# sure I don't have off-by-one errors\n",
    "# verified\n",
    "temp['Start'] = temp.Start-2\n",
    "temp['End'] = temp.Start+1\n",
    "\n",
    "# verified https://trello.com/c/fMhwX3s6\n",
    "temp.loc[temp.sj_loc=='start', 'Start'] = temp.loc[temp.sj_loc=='start', 'Start']-9\n",
    "temp.loc[temp.sj_loc=='start', 'End'] = temp.loc[temp.sj_loc=='start', 'End']+2\n",
    "\n",
    "\n",
    "temp.loc[temp.sj_loc=='end', 'End'] = temp.loc[temp.sj_loc=='end', 'End']+11\n",
    "temp.loc[temp.sj_loc=='end', 'Start'] = temp.loc[temp.sj_loc=='end', 'Start']\n",
    "\n",
    "temp.drop(['cell_line_id', 'map_genome'], axis=1, inplace=True)\n",
    "\n",
    "temp['len'] = temp.End-temp.Start\n",
    "assert len(temp.loc[temp.len!=12]) == 0\n",
    "\n",
    "temp = pr.PyRanges(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0df0dcb8-62a3-455d-a6cd-75ffdcb9661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_bed('test_4.bed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ceb679-6a3e-4868-8820-3947d2f15172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
