import pandas as pd
import os
import sys

p = os.path.dirname(os.path.dirname(os.getcwd()))+'/scripts/'
sys.path.append(p)

from sm_utils import *
from utils import *
# from vcf_utils import *

c_dir = '../common/'

meta_file = '../config.tsv'
meta_file_2 = 'config.tsv'
genomes_file = 'genomes_config.tsv'
configfile: '../config.yml'

include: f'{c_dir}download.smk'
include: f'{c_dir}samtools.smk'
include: f'{c_dir}winnowmap.smk'
include: f'{c_dir}bigwig.smk'
include: f'{c_dir}variant_calling.smk'
include: f'{c_dir}formatting.smk'
include: f'{c_dir}phasing.smk'
include: f'{c_dir}cerberus.smk'
include: f'{c_dir}bedtools.smk'
include: f'{c_dir}transdecoder.smk'
include: f'{c_dir}protein.smk'
include: f'{c_dir}minimap2.smk'
include: f'{c_dir}stats.smk'


df = parse_config(meta_file)
df2 = pd.read_csv(meta_file_2, sep='\t')
df2['tech_rep'] = df2.cell_line_id+'_1'

# TODO test
# df2 = df2.loc[df2.cell_line_id == 'GM24385']

# get the genomes to download
g_df = pd.read_csv(genomes_file, sep='\t')

# maternal haplotypes
g_df['aws_mat_link'] = g_df['hap2_aws_fasta']
g_df = g_df.loc[g_df['aws_mat_link'].notnull()]
assert len(g_df.loc[g_df['aws_mat_link'].str.contains('maternal')].index) == len(g_df.index)

# paternal haplotypes
g_df['aws_pat_link'] = g_df['hap1_aws_fasta']
g_df = g_df.loc[g_df['aws_pat_link'].notnull()]
assert len(g_df.loc[g_df['aws_pat_link'].str.contains('paternal')].index) == len(g_df.index)


genome_cols = ['same_population_sample', 'european_sample',	'afr_sample']
assemblies = genome_cols
g_df = g_df.loc[(g_df['sample'].isin(df2[genome_cols[0]]))|
                (g_df['sample'].isin(df2[genome_cols[1]]))|
                (g_df['sample'].isin(df2[genome_cols[2]]))]

# a little more df2 formatting
df2 = df2[['tech_rep', 'same_population_sample',
           'european_sample', 'afr_sample']].melt(id_vars='tech_rep')
df2 = df2.reset_index()
df2 = df2.rename({'variable':'assembly_status',
                  'value': 'assembly_sample'},
                  axis=1)

# limit just to the samples where we'll do this
df = df.loc[df.tech_rep.isin(df2.tech_rep.tolist())]

# get a key for assembly status, assembly sample, and actual sample
df2['dataset_key'] = df2.assembly_status+'_'+\
                     df2.assembly_sample+'_'+\
                     df2.tech_rep


wildcard_constraints:
    assembly='|'.join([re.escape(x) for x in g_df['sample'].tolist()]),
    sample='|'.join([re.escape(x) for x in df.tech_rep.tolist()]),
    lab_rep='|'.join([re.escape(x) for x in df.lab_rep.tolist()]),

rule all:
    input:
        expand(config['ref']['alt']['personal']['fa_gz'],
               assembly_sample=g_df['sample'].tolist())

    # input:
    #     expand(config['lr']['map_personal']['max_mapq_tsv'],
    #                        sample='GM24385_1',
    #                        thresh=0)

        # expand(config['lr']['map_personal']['mapq_tsv_summary'],
        #            thresh=[0, 10]),
        # expand(config['lr']['map_personal']['max_mapq_tsv_summary'],
        #        thresh=[0, 10]),


        # expand(config['ref']['alt']['personal']['pat_fa_gz'],
        #        assembly_sample=g_df['sample'].tolist())
        # expand(config['lr']['map_personal']['bam_ind'],
        #        zip,
        #        assembly_status=df2.assembly_status.tolist(),
        #        assembly_sample=df2.assembly_sample.tolist(),
        #        sample=df2.tech_rep.tolist()),
        # config['lr']['fastq_n_reads_summary'],
        # expand(config['lr']['map_personal']['bam_n_reads_summary'],
        #        zip,
        #        assembly_status=df2.assembly_status.tolist(),
        #        assembly_sample=df2.assembly_sample.tolist(),
        #        sample=df2.tech_rep.tolist()),
        # expand(config['lr']['map_personal']['bam_mapqs_summary'],
        #     zip,
        #     assembly_status=df2.assembly_status.tolist(),
        #     assembly_sample=df2.assembly_sample.tolist(),
        #     sample=df2.tech_rep.tolist()),
        # expand(config['lr']['map_personal']['bam_query_covs_summary'],
        #      zip,
        #      assembly_status=df2.assembly_status.tolist(),
        #      assembly_sample=df2.assembly_sample.tolist(),
        #      sample=df2.tech_rep.tolist()),



def get_df_val(df, col1, col_dict):
    temp = df.copy(deep=True)

    for key, item in col_dict.items():
        temp = temp.loc[temp[key] == item]

    val = temp[col1].unique()
    assert len(val) == 1
    return val[0]

use rule dl_aws as dl_personal_assembly_mat with:
    params:
        link = lambda wc: get_df_val(g_df, 'aws_mat_link',
                            {'sample': wc.assembly_sample})
    output:
        out = config['ref']['alt']['personal']['fa_gz']

use rule dl_aws as dl_personal_assembly_pat with:
    params:
        link = lambda wc: get_df_val(g_df, 'aws_pat_link',
                            {'sample': wc.assembly_sample})
    output:
        out = config['ref']['alt']['personal']['pat_fa_gz']

# rule get_y_chr_fa:
    # input:
    #     fa = config['ref']['alt']['personal']['pat_fa_gz']
    # resources:
    #     threads = 1,
    #     nodes = 1
    # output:
    #     fa = config['ref']['alt']['personal']['pat_y_fa_gz']
    # shell:
    #     """
    #     module load intel/2023.0
    #     module load samtools
    #     samtools faidx <(zcat {input.fa}) chrX | gzip > {output.fa}
    #     """


### index genome
use rule minimap2_index as ind_alt_assembly with:
    input:
        fa = config['ref']['alt']['personal']['fa_gz']
    output:
        ind = config['ref']['alt']['personal']['fa_mmi']

use rule minimap2_with_index as alt_map with:
    input:
        fq = lambda wc: expand(config['lr']['fastq'],
                    lab_sample=get_df_val(df,
                            'lab_rep',
                            {'tech_rep': wc.sample})),
        ind = config['ref']['alt']['personal']['fa_mmi']
    resources:
        threads = 8,
        nodes = 32
    output:
        sam = temporary(config['lr']['map_personal']['sam'])

use rule filt_non_prim_unmap_supp as alt_filt_map with:
    input:
        align = config['lr']['map_personal']['sam']
    output:
        align = temporary(config['lr']['map_personal']['sam_filt'])

use rule sam_to_bam as alt_sam_to_bam with:
    input:
        sam = config['lr']['map_personal']['sam_filt']
    output:
        bam = temporary(config['lr']['map_personal']['bam'])

# index and sort bam
use rule sort_bam as alt_sort_bam with:
    input:
        bam = config['lr']['map_personal']['bam']
    output:
        bam = config['lr']['map_personal']['bam_sort']

use rule index_bam as alt_index_bam with:
    input:
        bam = config['lr']['map_personal']['bam_sort']
    output:
        ind = config['lr']['map_personal']['bam_ind']

# ### statistics
# rule all_stats:
#     input:
#         config['lr']['fastq_n_reads_summary'],
#         expand(config['lr']['map_personal']['bam_n_reads_summary'],
#                assembly=assemblies),
#         expand(config['lr']['map_personal']['bam_mapqs_summary'],
#               assembly=assemblies)


# raw fq counts
use rule fq_count_reads as count_raw_reads with:
    input:
        fq = lambda wc: expand(config['lr']['fastq'],
                    lab_sample=get_df_val(df,
                            'lab_rep',
                            {'tech_rep': wc.sample}))
    output:
        txt = config['lr']['fastq_n_reads']

use rule count_reads_summary as fq_count_reads_summary with:
    input:
        txts = lambda wc: expand(config['lr']['fastq_n_reads'],
                                sample=df2['tech_rep'].tolist()),
    params:
        samples = df2['tech_rep'].tolist()
    output:
        summ = config['lr']['fastq_n_reads_summary']

# filtered bam counts
use rule count_bam as alt_count_mapped_reads with:
    input:
        align = config['lr']['map_personal']['bam_sort']
    output:
        txt = config['lr']['map_personal']['bam_n_reads']


use rule count_reads_summary as sam_count_reads_summary with:
    input:
        txts = lambda wc: expand(config['lr']['map_personal']['bam_n_reads'],
                                       zip,
                                       assembly_status=df2.assembly_status.tolist(),
                                       assembly_sample=df2.assembly_sample.tolist(),
                                       sample=df2.tech_rep.tolist())
    params:
        samples = df2['dataset_key'].tolist()
    output:
        summ = config['lr']['map_personal']['bam_n_reads_summary']

# mapqs per read
use rule bam_get_mapqs as alt_get_mapqs with:
    input:
        bam = config['lr']['map_personal']['bam_sort']
    output:
        txt = config['lr']['map_personal']['bam_mapqs']

use rule read_id_df_summary as alt_get_mapqs_summary with:
    input:
        tsvs = lambda wc: expand(config['lr']['map_personal']['bam_mapqs'],
                               zip,
                               assembly_status=df2.assembly_status.tolist(),
                               assembly_sample=df2.assembly_sample.tolist(),
                               sample=df2.tech_rep.tolist())
    params:
        samples = df2['dataset_key'].tolist()
    resources:
        threads = 1,
        nodes = 3
    output:
        summ = config['lr']['map_personal']['bam_mapqs_summary']

# query coverage and read length per read
use rule bam_get_query_cov as alt_get_qcov with:
    input:
        align = config['lr']['map_personal']['bam_sort']
    output:
        out = config['lr']['map_personal']['bam_query_covs']

use rule read_id_df_summary as alt_get_qcov_summary with:
    input:
        tsvs = lambda wc: expand(config['lr']['map_personal']['bam_query_covs'],
                               zip,
                               assembly_status=df2.assembly_status.tolist(),
                               assembly_sample=df2.assembly_sample.tolist(),
                               sample=df2.tech_rep.tolist())
    params:
        samples = df2['dataset_key'].tolist()
    output:
        summ = config['lr']['map_personal']['bam_query_covs_summary']

################################
#### mapping summary statistics that aren't so heavy
################################
use rule bool_mapq_personal_mappings_summary as get_bool_mapq_personal_mappings_summary with:
    input:
        files = lambda wc: expand(expand(config['lr']['map_personal']['bam_mapqs'],
                           zip,
                           assembly_status=assemblies,
                           assembly_sample=df2.assembly_sample.tolist(),
                           allow_missing=True),
                           sample=wc.sample)
    params:
        assemblies = assemblies,
        mapq_thresh = lambda wc: wc.thresh,
    output:
        tsv = config['lr']['map_personal']['mapq_tsv'],
        same_pop_uniq_reads = config['lr']['map_personal']['same_pop_uniq_reads'],
        upset = config['lr']['map_personal']['mapq_upset']

use rule max_mapq_personal_mappings_summary as get_max_mapq_personal_mappings_summary with:
    input:
        files = lambda wc: expand(expand(config['lr']['map_personal']['bam_mapqs'],
                           zip,
                           assembly_status=assemblies,
                           assembly_sample=df2.assembly_sample.tolist(),
                           allow_missing=True),
                           sample=wc.sample)
    params:
        assemblies = assemblies,
        mapq_thresh = lambda wc: wc.thresh
    output:
        tsv = config['lr']['map_personal']['max_mapq_tsv'],
        same_pop_uniq_reads = config['lr']['map_personal']['max_same_pop_uniq_reads'],
        upset = config['lr']['map_personal']['max_mapq_upset']



use rule read_id_df_summary as get_mapq_summary_summary with:
    input:
        tsvs = lambda wc: expand(config['lr']['map_personal']['mapq_tsv'],
                               zip,
                               assembly_status=df2.assembly_status.tolist(),
                               assembly_sample=df2.assembly_sample.tolist(),
                               sample=df2.tech_rep.tolist(),
                               thresh=wc.thresh)
    resources:
        threads = 1,
        nodes = 3
    params:
        samples = df2['tech_rep'].tolist()
    output:
        summ = config['lr']['map_personal']['mapq_tsv_summary']

use rule read_id_df_summary as get_max_mapq_summary_summary with:
    input:
        tsvs = lambda wc: expand(config['lr']['map_personal']['max_mapq_tsv'],
                               zip,
                               assembly_status=df2.assembly_status.tolist(),
                               assembly_sample=df2.assembly_sample.tolist(),
                               sample=df2.tech_rep.tolist(),
                               thresh=wc.thresh)
    resources:
        threads = 1,
        nodes = 3
    params:
        samples = df2['tech_rep'].tolist()
    output:
        summ = config['lr']['map_personal']['max_mapq_tsv_summary']
