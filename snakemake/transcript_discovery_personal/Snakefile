import pandas as pd
import os
import sys
import pysam
import pyranges as pr

p = os.path.dirname(os.path.dirname(os.getcwd()))+'/scripts/'
sys.path.append(p)

from sm_utils import *
from utils import *
# from vcf_utils import *

c_dir = '../common/'

meta_file = '../config.tsv'
configfile: '../config.yml'

include: f'{c_dir}download.smk'
include: f'{c_dir}samtools.smk'
include: f'{c_dir}winnowmap.smk'
include: f'{c_dir}bigwig.smk'
include: f'{c_dir}variant_calling.smk'
include: f'{c_dir}formatting.smk'
include: f'{c_dir}phasing.smk'
include: f'{c_dir}cerberus.smk'
include: f'{c_dir}bedtools.smk'
include: f'{c_dir}transdecoder.smk'
include: f'{c_dir}protein.smk'
include: f'{c_dir}minimap2.smk'
include: f'{c_dir}stats.smk'
include: f'{c_dir}lr-kallisto.smk'

# sample information
# sample information
df = load_meta()
df = df.loc[~df['sample'].str.contains('_')]
df['lab_sample'] = df['lab_number_sample'].astype(str)+'_'+\
                      df['lab_sampleid'].astype(str)+'_'+\
                      df['cell_line_id'].astype(str)
df = df[['cell_line_id', 'sample', 'hapmap_DNA_ID', 'lab_sample']].drop_duplicates()

temp_df = pd.read_csv('cell_line_ids.txt', header=None, names=['cell_line_id'])

# make a 1000g cell line id col
df['cell_line_id_1000g'] = df.cell_line_id

inds = df.loc[~df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())].index
df.loc[inds, 'cell_line_id_1000g'] = df.loc[inds, 'hapmap_DNA_ID']
len(df.index)

# limit to just those in 1000g
df = df.loc[df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())]
assert len(df.index) == 30

# # TODO removing one problematic sample
# bad_samples = ['NA19328']
# df = df.loc[df.cell_line_id_1000g.isin(bad_samples)]

# # TODO try on just one cell line
# test_samples = ['NA12273']
# df = df.loc[df.cell_line_id_1000g.isin(test_samples)]

hap = ['hap1', 'hap2']

# genome information table
sqanti_genomes = ['hap1', 'hap2', 'hg38']
sqanti_df = df.copy(deep=True)
sqanti_df['sqanti_genome'] = [sqanti_genomes] * len(df)
sqanti_df = sqanti_df.explode('sqanti_genome').reset_index(drop=True)

# get all of the IDs for the 1000g
df_1kg = pd.read_csv(config['1000g']['meta'],
    sep='\t', comment='#', header=None)
df_1kg = df_1kg[[9,10]]
df_1kg.columns = ['cell_line_id', 'population']
len(df_1kg.cell_line_id.unique())
temp = pd.read_csv(config['1000g']['trios_meta'],
    sep='\t', comment='#', header=None)
temp = temp[[9,10]]
temp.columns = ['cell_line_id', 'population']
df_1kg = pd.concat([df_1kg, temp], axis=0)
len(df_1kg.cell_line_id.unique())
assert len(df_1kg.cell_line_id.unique()) == 3202

wildcard_constraints:
    cell_line_id='|'.join([re.escape(x) for x in df['cell_line_id_1000g'].tolist()]),
    hap='|'.join([re.escape(x) for x in hap]),
    ref='|'.join([re.escape(x) for x in sqanti_genomes]),

rule all:
    input:
        expand(config['lr']['td_personal']['exon_vars']['sj_10nt_vcf_intersect'],
               cell_line_id=df['cell_line_id_1000g'].tolist()),
        expand(config['lr']['td_personal']['exon_vars']['sj_12nt_vcf_intersect'],
               cell_line_id=df['cell_line_id_1000g'].tolist())

        # expand(config['lr']['td_personal']['exon_vars']['sj_12nt_vcf_intersect_vcf'],
        #        cell_line_id=df_1kg['cell_line_id'].tolist()), # this guy for all the samples in here

        # config['lr']['td_personal']['sqanti']['sj_10nt_bed']
        # expand(config['lr']['td_personal']['cerb']['ic_parsed'],
        #                    cell_line_id=df['cell_line_id_1000g'].tolist(),
        #                    hap=hap,
        #                    sqanti_genome=sqanti_genomes),
        # expand(config['lr']['espresso']['cerb']['ic_parsed'],
        #                cell_line_id=df['cell_line_id_1000g'].tolist(),
        #                sqanti_genome=sqanti_genomes),
        # expand(config['lr']['td_personal']['map']['bam_ind'],
        #        cell_line_id=df['cell_line_id_1000g'].tolist(),
        #        hap=hap),
        # config['lr']['td_personal']['sqanti']['sj_summary'],
        # config['lr']['td_personal']['sqanti']['cl_summary'],
        # config['lr']['td_personal']['cerb']['ic_summary'],
        # expand(config['lr']['espresso']['sqanti']['sjs'],
        #        cell_line_id=df['cell_line_id_1000g'].tolist()),
        # expand(config['lr']['espresso']['cerb']['ics'],
        #        cell_line_id=df['cell_line_id_1000g'].tolist()),
        # expand(config['lr']['td_personal']['sqanti']['sjs'],
        #        cell_line_id=df['cell_line_id_1000g'].tolist(),
        #        hap=hap),
        # expand(config['lr']['td_personal']['cerb']['ics'],
        #        cell_line_id=df['cell_line_id_1000g'].tolist(),
        #        hap=hap),


def get_df_val(df, col1, col_dict):
    temp = df.copy(deep=True)

    for key, item in col_dict.items():
        temp = temp.loc[temp[key] == item]

    val = temp[col1].unique()
    assert len(val) == 1
    return val[0]

### index genome
use rule minimap2_index as ind_alt_assembly with:
    input:
        fa = config['lr']['td_personal']['ref_fa']
    output:
        ind = config['lr']['td_personal']['ref_fa_mmi']

rule minimap2_with_index_2:
    resources:
        threads = 8,
        nodes = 32
    shell:
        """
        module load minimap2
        minimap2 \
            -ax splice \
            -t {resources.threads} \
            --MD \
            --secondary=no \
            -L \
            -o {output.sam} \
            -a {input.ind} \
            {input.fq}
        """

use rule minimap2_with_index_2 as alt_map with:
    input:
        fq = lambda wc: expand(config['lr']['q10_fastq'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        ind = config['lr']['td_personal']['ref_fa_mmi']
    resources:
        threads = 8,
        nodes = 32
    output:
        sam = temporary(config['lr']['td_personal']['map']['sam'])
        # sam = temporary(config['lr']['td_personal']['map']['sam'])

use rule filt_non_prim_unmap_supp as alt_filt_map with:
    input:
        align = config['lr']['td_personal']['map']['sam']
    output:
        align = temporary(config['lr']['td_personal']['map']['sam_filt'])

use rule sam_to_bam as alt_sam_to_bam with:
    input:
        sam = config['lr']['td_personal']['map']['sam_filt']
    output:
        bam = temporary(config['lr']['td_personal']['map']['bam'])

# index and sort bam
use rule sort_bam as alt_sort_bam with:
    input:
        bam = config['lr']['td_personal']['map']['bam']
    output:
        bam = config['lr']['td_personal']['map']['bam_sort']

use rule index_bam as alt_index_bam with:
    input:
        bam = config['lr']['td_personal']['map']['bam_sort']
    output:
        ind = config['lr']['td_personal']['map']['bam_ind']

# ### statistics
# rule all_stats:
#     input:
#         config['lr']['fastq_n_reads_summary'],
#         expand(config['lr']['td_personal']['map']['bam_n_reads_summary'],
#                assembly=assemblies),
#         expand(config['lr']['td_personal']['map']['bam_mapqs_summary'],
#               assembly=assemblies)
#
# # raw fq read ids
# use rule fq_get_read_ids as fq_read_ids with:
#     input:
#         fq = lambda wc: expand(config['lr']['q10_fastq'],
#                     lab_sample=get_df_val(df,
#                             'lab_rep',
#                             {'tech_rep': wc.sample}))
#     output:
#         txt = config['lr']['fastq_reads']
#
# # raw fq counts
# use rule fq_count_reads as count_raw_reads with:
#     input:
#         fq = lambda wc: expand(config['lr']['q10_fastq'],
#                     lab_sample=get_df_val(df,
#                             'lab_rep',
#                             {'tech_rep': wc.sample}))
#     output:
#         txt = config['lr']['fastq_n_reads']
#
# use rule count_reads_summary as fq_count_reads_summary with:
#     input:
#         txts = lambda wc: expand(config['lr']['fastq_n_reads'],
#                                 sample=df['tech_rep'].tolist()),
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['fastq_n_reads_summary']
#
# # filtered bam counts
# use rule count_bam as alt_count_mapped_reads with:
#     input:
#         align = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         txt = config['lr']['td_personal']['map']['bam_n_reads']
#
#
# use rule count_reads_summary as sam_count_reads_summary with:
#     input:
#         txts = lambda wc: expand(config['lr']['td_personal']['map']['bam_n_reads'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist()),
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_n_reads_summary']
#
# # mapqs per read
# use rule bam_get_mapqs as alt_get_mapqs with:
#     input:
#         bam = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         txt = config['lr']['td_personal']['map']['bam_mapqs']
#
# use rule read_id_df_summary as alt_get_mapqs_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist())
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_mapqs_summary']
#
# # query coverage and read length per read
# use rule bam_get_query_cov as alt_get_qcov with:
#     input:
#         align = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         out = config['lr']['td_personal']['map']['bam_query_covs']
#
# use rule read_id_df_summary as alt_get_qcov_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['bam_query_covs'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist())
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_query_covs_summary']
#
# ################################
# #### mapping summary statistics that aren't so heavy
# ################################
# use rule bool_mapq_summary as get_bool_mapq_summary with:
#     input:
#         files = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                               sample=wc.sample,
#                               assembly=assemblies)
#     params:
#         assemblies = assemblies,
#         mapq_thresh = lambda wc: wc.thresh
#     output:
#         tsv = config['lr']['td_personal']['map']['mapq_tsv'],
#         afr_reads = config['lr']['td_personal']['map']['mapq_uniq_afr_reads'],
#         upset = config['lr']['td_personal']['map']['mapq_upset']
#
# use rule max_mapq_summary as get_max_mapq_summary with:
#     input:
#         files = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                               sample=wc.sample,
#                               assembly=assemblies)
#     params:
#         assemblies = assemblies,
#         mapq_thresh = lambda wc: wc.thresh
#     output:
#         tsv = config['lr']['td_personal']['map']['max_mapq_tsv'],
#         afr_reads = config['lr']['td_personal']['map']['max_mapq_uniq_afr_reads'],
#         upset = config['lr']['td_personal']['map']['max_mapq_upset']
#
#
#
# use rule read_id_df_summary as get_mapq_summary_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['mapq_tsv'],
#                sample=df.tech_rep.tolist(),
#                thresh=wc.thresh)
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['mapq_tsv_summary']
#
# use rule read_id_df_summary as get_max_mapq_summary_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['max_mapq_tsv'],
#                sample=df.tech_rep.tolist(),
#                thresh=wc.thresh)
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['max_mapq_tsv_summary']


#############
##### sqanti
############


rule sqanti:
    resources:
        nodes = 2,
        threads = 8
    conda:
        'base'
    shell:
        """
        conda activate /gpfs/projects/bsc83/utils/conda_envs/SQANTI3-5.2.1
        mkdir -p {params.odir}
        python {params.sq_path}sqanti3_qc.py \
            {input.gtf} \
            {input.annot} \
            {input.fa} \
            -d {params.odir} \
            --report skip \
            --force_id_ignore \
            --aligner_choice minimap2 \
            --skipORF \
            -o {params.opref}
        """

# # get ics file
# use rule gtf_to_ic as gc_gtf_to_ic with:
#     input:
#         gtf = config['lr']['td_personal']['espresso']['gtf']
#     output:
#         tsv = config['lr']['td_personal']['cerb']['ics']

##################
###### stuff for espresso w/ normal hg38

# # get ics file
# use rule gtf_to_ic as gc_gtf_to_ic_hg38 with:
#     input:
#         gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
#                        lab_sample=get_df_val(df,
#                             'lab_sample',
#                             {'cell_line_id_1000g': wc.cell_line_id})),
#     output:
#         tsv = config['lr']['espresso']['cerb']['ics']


##################
###### stuff for espresso w/ personal genome haplotypes

# use rule sqanti as sqanti_td_td with:
#     input:
#         gtf = config['lr']['td_personal']['espresso']['gtf'],
#         fa = config['lr']['td_personal']['ref_fa'],
#         annot = config['ref']['gtf']
#     params:
#         sq_path = config['software']['sqanti_path'],
#         opref = config['lr']['td_personal']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
#         odir = '/'.join(config['lr']['td_personal']['sqanti']['gff'].split('/')[:-1])+'/'
#     output:
#         gff = config['lr']['td_personal']['sqanti']['gff'],
#         sjs = config['lr']['td_personal']['sqanti']['sjs'],
#         cl = config['lr']['td_personal']['sqanti']['cl'],
#
#
# use rule sqanti as sqanti_td_hg38 with:
#     input:
#         gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
#                        lab_sample=get_df_val(df,
#                             'lab_sample',
#                             {'cell_line_id_1000g': wc.cell_line_id})),
#         fa = config['ref']['fa'],
#         annot = config['ref']['gtf']
#     params:
#         sq_path = config['software']['sqanti_path'],
#         opref = config['lr']['espresso']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
#         odir = '/'.join(config['lr']['espresso']['sqanti']['gff'].split('/')[:-1])+'/'
#     output:
#         gff = config['lr']['espresso']['sqanti']['gff'],
#         sjs = config['lr']['espresso']['sqanti']['sjs'],
#         cl = config['lr']['espresso']['sqanti']['cl']

####################
# summarize splice junctions / ics
# rule summarize:
#     resources:
#         nodes = 1,
#         threads = 1
#     run:
#         hg38_files = list(input.hg38_files)
#         td_hap1_files = list(input.td_hap1_files)
#         td_hap2_files = list(input.td_hap2_files)
#
#         genomes = ['hg38', 'hap1', 'hap2']
#
#         df = pd.DataFrame()
#         for genome, files in zip(genomes, [hg38_files, td_hap1_files, td_hap2_files]):
#             for clid, file in zip(params.cell_line_id, files):
#                 # quick check to make sure we're looking at the right
#                 # cell line for each file
#                 assert clid in file
#                 temp = pd.read_csv(file, sep='\t')
#                 temp['genome'] = genome
#                 temp['cell_line_id'] = clid
#                 df = pd.concat([df, temp], axis=0)
#         df.to_csv(output.summary, sep='\t')
#
# use rule summarize as sj_summary with:
#     input:
#         hg38_files = expand(config['lr']['espresso']['sqanti']['sjs'],
#                cell_line_id=df['cell_line_id_1000g'].tolist(),
#                sqanti_genome=sqanti_genomes),
#         td_hap1_files = expand(config['lr']['td_personal']['sqanti']['sjs'],
#                    cell_line_id=df['cell_line_id_1000g'].tolist(),
#                    hap='hap1',
#                    sqanti_genome=sqanti_genomes),
#         td_hap2_files = expand(config['lr']['td_personal']['sqanti']['sjs'],
#                   cell_line_id=df['cell_line_id_1000g'].tolist(),
#                   hap='hap2',
#                   sqanti_genome=sqanti_genomes),
#     resources:
#         nodes = 3,
#         threads = 1
#     params:
#         cell_line_id = df['cell_line_id_1000g'].tolist()
#     output:
#         summary = config['lr']['td_personal']['sqanti']['sj_summary']
#
# use rule summarize as ic_summary with:
#     input:
#         hg38_files = expand(config['lr']['espresso']['cerb']['ics'],
#                cell_line_id=df['cell_line_id_1000g'].tolist()),
#         td_hap1_files = expand(config['lr']['td_personal']['cerb']['ics'],
#                    cell_line_id=df['cell_line_id_1000g'].tolist(),
#                    hap='hap1'),
#         td_hap2_files = expand(config['lr']['td_personal']['cerb']['ics'],
#                   cell_line_id=df['cell_line_id_1000g'].tolist(),
#                   hap='hap2'),
#     params:
#         cell_line_id = df['cell_line_id_1000g'].tolist()
#     output:
#         summary = config['lr']['td_personal']['cerb']['ic_summary']
#
# use rule summarize as cl_summary with:
#     input:
#         hg38_files = expand(config['lr']['espresso']['sqanti']['cl'],
#                cell_line_id=df['cell_line_id_1000g'].tolist()),
#         td_hap1_files = expand(config['lr']['td_personal']['sqanti']['cl'],
#                    cell_line_id=df['cell_line_id_1000g'].tolist(),
#                    hap='hap1'),
#         td_hap2_files = expand(config['lr']['td_personal']['sqanti']['cl'],
#                   cell_line_id=df['cell_line_id_1000g'].tolist(),
#                   hap='hap2'),
#     resources:
#         nodes = 3,
#         threads = 1
#     params:
#         cell_line_id = df['cell_line_id_1000g'].tolist()
#     output:
#         summary = config['lr']['td_personal']['sqanti']['cl_summary']
#


def get_sqanti_ref_fa(wc):
    if wc['sqanti_genome'] == 'hg38':
        return config['ref']['fa']
    else:
        return expand(config['lr']['td_personal']['ref_fa'],
                        cell_line_id=wc['cell_line_id'],
                        hap=wc['sqanti_genome'])[0]



# just for the hg38-mapped reads
use rule sqanti as sqanti_td_hg38 with:
    input:
        gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        fa = lambda wc: get_sqanti_ref_fa(wc),
        annot = config['ref']['gtf']
    params:
        sq_path = config['software']['sqanti_path'],
        opref = config['lr']['espresso']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
        odir = '/'.join(config['lr']['espresso']['sqanti']['gff'].split('/')[:-1])+'/'
    output:
        gff = config['lr']['espresso']['sqanti']['gff'],
        sjs = config['lr']['espresso']['sqanti']['sjs'],
        cl = config['lr']['espresso']['sqanti']['cl']

# sqanti for td mapped genomes
use rule sqanti as sqanti_td_td with:
    input:
        gtf = config['lr']['td_personal']['espresso']['gtf'],
        fa = lambda wc: get_sqanti_ref_fa(wc),
        annot = config['ref']['gtf']
    params:
        sq_path = config['software']['sqanti_path'],
        opref = config['lr']['td_personal']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
        odir = '/'.join(config['lr']['td_personal']['sqanti']['gff'].split('/')[:-1])+'/'
    output:
        gff = config['lr']['td_personal']['sqanti']['gff'],
        sjs = config['lr']['td_personal']['sqanti']['sjs'],
        cl = config['lr']['td_personal']['sqanti']['cl'],

################
### summaries for each boi
###############
rule summarize_one_exp:
    resources:
        threads = 1,
        nodes = 1
    conda:
        'cerberus'
    shell:
        """
        python {params.scripts_dir}get_ic_sj_td_summaries.py \
          --cl_file {input.cl} \
          --gtf_file {input.gtf} \
          --sjs_file {input.sjs} \
          --sjs_summary {output.sj_summary} \
          --ic_summary {output.ic_summary} \
          --cell_line_id {params.cell_line_id} \
          --map_genome {params.hap} \
          --sqanti_genome {params.sqanti_genome}
        """

use rule summarize_one_exp as td_summarize_one_exp with:
    input:
        gtf = config['lr']['td_personal']['espresso']['gtf'],
        sjs = config['lr']['td_personal']['sqanti']['sjs'],
        cl = config['lr']['td_personal']['sqanti']['cl']
    params:
        cell_line_id = lambda wc: wc.cell_line_id,
        hap = lambda wc: wc.hap,
        sqanti_genome = lambda wc: wc.sqanti_genome,
        scripts_dir = p
    output:
        sj_summary = config['lr']['td_personal']['sqanti']['sj_parsed'],
        ic_summary = config['lr']['td_personal']['cerb']['ic_parsed']

use rule summarize_one_exp as hg38_summarize_one_exp with:
    input:
        gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        sjs = config['lr']['espresso']['sqanti']['sjs'],
        cl = config['lr']['espresso']['sqanti']['cl']
    params:
        cell_line_id = lambda wc: wc.cell_line_id,
        hap = 'hg38',
        sqanti_genome = lambda wc: wc.sqanti_genome,
        scripts_dir = p
    output:
        sj_summary = config['lr']['espresso']['sqanti']['sj_parsed'],
        ic_summary = config['lr']['espresso']['cerb']['ic_parsed']

rule create_summaries:
    input:
        ic_files = expand(config['lr']['td_personal']['cerb']['ic_parsed'],
                           cell_line_id=df['cell_line_id_1000g'].tolist(),
                           hap=hap,
                           sqanti_genome=sqanti_genomes)+\
                    expand(config['lr']['espresso']['cerb']['ic_parsed'],
                                   cell_line_id=df['cell_line_id_1000g'].tolist(),
                                   sqanti_genome=sqanti_genomes),
        sj_files = expand(config['lr']['td_personal']['sqanti']['sj_parsed'],
                           cell_line_id=df['cell_line_id_1000g'].tolist(),
                           hap=hap,
                           sqanti_genome=sqanti_genomes)+\
                    expand(config['lr']['espresso']['sqanti']['sj_parsed'],
                                   cell_line_id=df['cell_line_id_1000g'].tolist(),
                                   sqanti_genome=sqanti_genomes)
    resources:
        threads = 1,
        nodes = 4
    output:
        ic_summary = config['lr']['td_personal']['cerb']['ic_summary'],
        sj_summary = config['lr']['td_personal']['sqanti']['sj_summary']
    run:
        df = pd.DataFrame()
        for f in list(input.ic_files):
            temp = pd.read_csv(f, sep='\t')
            df = pd.concat([temp, df], axis=0)
        df.to_csv(output.ic_summary)

        df = pd.DataFrame()
        for f in list(input.sj_files):
            temp = pd.read_csv(f, sep='\t')
            df = pd.concat([temp, df], axis=0)
        df.to_csv(output.sj_summary)

# get a bed file of +-10 bp from each SJ
rule get_10nt_sj_bed:
    input:
        sj_summary = config['lr']['td_personal']['sqanti']['sj_summary']
    resources:
        threads = 1,
        nodes = 2
    output:
        bed = config['lr']['td_personal']['sqanti']['sj_10nt_bed']
    run:
        df = pd.read_csv(input.sj_summary)
        df.drop('Unnamed: 0', axis=1, inplace=True)

        # remove sqanti genome and drop dupes
        # the sqanti genome / sqanti metrics SHOULD be irrelevant here
        df = df.drop(['sqanti_genome', 'canonical', 'splice_motif'], axis=1)
        print(len(df.index))
        df = df.drop_duplicates()
        print(len(df.index))

        # then make sure that there are no dupe. sj+sj nov+sample+map genome
        temp = df.loc[df[['sj_id', 'junction_novelty',
                          'cell_line_id', 'map_genome', 'start_site_novelty',
                          'end_site_category']].duplicated(keep=False)]
        assert len(temp.index) == 0
        del temp

        df.rename({'end_site_category': 'end_site_novelty'}, axis=1, inplace=True)

        # transform to be t/f for each ic per genome
        temp = pd.crosstab(index=[df.sj_id, df.junction_novelty,
                                         df.start_site_novelty,
                                         df.end_site_novelty, df.cell_line_id],
                                  columns=df.map_genome,
                                  values=df.map_genome,
                                  aggfunc=lambda x: True).fillna(False).reset_index()

        temp[['Chromosome', 'Strand', 'Start', 'End']] = temp.sj_id.str.split('_', expand=True)
        temp = temp[['Chromosome', 'Strand', 'Start', 'End',
                     'cell_line_id', 'sj_id']].drop_duplicates()

        temp.Start = temp.Start.astype(int)
        temp.End = temp.End.astype(int)
        assert len(temp.loc[temp.Start>temp.End])==0

        # melt to 5' and 3'
        temp = temp.melt(id_vars=['Chromosome', 'Strand', 'cell_line_id', 'sj_id'],
                         value_vars=['Start', 'End'])
        temp['sj_loc'] = ''
        temp.loc[temp.map_genome=='Start', 'sj_loc'] = 'start'
        temp.loc[temp.map_genome=='End', 'sj_loc'] = 'end'

        temp.rename({'value':'Start'}, axis=1, inplace=True)
        # need to verify that this is working using like one motif or something make
        # sure I don't have off-by-one errors
        # verified
        temp['Start'] = temp.Start-2
        temp['End'] = temp.Start+1

        # add 10 to start, rm 10 from end
        # verified https://trello.com/c/qzMAZpAm
        temp.loc[temp.sj_loc=='start', 'Start'] = temp.loc[temp.sj_loc=='start', 'Start']-9
        temp.loc[temp.sj_loc=='end', 'End'] = temp.loc[temp.sj_loc=='end', 'End']+11
        temp.loc[temp.sj_loc=='end', 'Start'] = temp.loc[temp.sj_loc=='end', 'Start']+2

        temp.drop(['cell_line_id', 'map_genome'], axis=1, inplace=True)

        temp['len'] = temp.End-temp.Start
        assert len(temp.loc[temp.len!=10]) == 0

        temp = pr.PyRanges(temp)

        temp.to_bed(output.bed)

# use rule bcftools_subset_on_samples as get_sample_exon_vcf with:
#     input:
#         vcf = config['1000g']['exon_vars_header']
#     params:
#         samples = lambda wc: wc.cell_line_id
#     output:
#         vcf = temporary(config['lr']['td_personal']['exon_vars']['vcf_sample'])

#
# use rule intersect_variants_with_bed as intersect_sj_10nt_vcf with:
#     input:
#         bed = config['lr']['td_personal']['sqanti']['sj_10nt_bed'],
#         vcf = config['lr']['td_personal']['exon_vars']['vcf_sample']
#     resources:
#         threads = 16,
#         nodes = 2
#     output:
        # bed = config['lr']['td_personal']['exon_vars']['sj_10nt_vcf_intersect']


rule bcftool_subset_on_samples_and_bed_intersect:
    resources:
        threads = 8,
        nodes = 2
    conda:
        'base'
    shell:
        """
        module load bedtools
        bcftools view \
            --samples {params.samples} \
            --threads {resources.threads} \
            -Ov \
            --min-ac=1 \
            {input.vcf} | \
        bedtools intersect \
             -a {input.bed} \
             -b stdin \
             -wa \
             -u > {output.bed}
        """

use rule bcftool_subset_on_samples_and_bed_intersect as sample_10bp_sj_var with:
    input:
        vcf = config['1000g']['exon_vars_header'],
        bed = config['lr']['td_personal']['sqanti']['sj_10nt_bed']
    params:
        samples = lambda wc: wc.cell_line_id
    output:
        bed = config['lr']['td_personal']['exon_vars']['sj_10nt_vcf_intersect']

# get a bed file of +-10 bp from each SJ
rule get_12nt_sj_ss_bed:
    input:
        sj_summary = config['lr']['td_personal']['sqanti']['sj_summary']
    resources:
        threads = 1,
        nodes = 2
    output:
        bed = config['lr']['td_personal']['sqanti']['sj_12nt_bed']
    run:
        df = pd.read_csv(input.sj_summary)
        df.drop('Unnamed: 0', axis=1, inplace=True)

        # remove sqanti genome and drop dupes
        # the sqanti genome / sqanti metrics SHOULD be irrelevant here
        df = df.drop(['sqanti_genome', 'canonical', 'splice_motif'], axis=1)
        print(len(df.index))
        df = df.drop_duplicates()
        print(len(df.index))

        # then make sure that there are no dupe. sj+sj nov+sample+map genome
        temp = df.loc[df[['sj_id', 'junction_novelty',
                          'cell_line_id', 'map_genome', 'start_site_novelty',
                          'end_site_category']].duplicated(keep=False)]
        assert len(temp.index) == 0
        del temp

        df.rename({'end_site_category': 'end_site_novelty'}, axis=1, inplace=True)

        # transform to be t/f for each ic per genome
        temp = pd.crosstab(index=[df.sj_id, df.junction_novelty,
                                         df.start_site_novelty,
                                         df.end_site_novelty, df.cell_line_id],
                                  columns=df.map_genome,
                                  values=df.map_genome,
                                  aggfunc=lambda x: True).fillna(False).reset_index()


        temp[['Chromosome', 'Strand', 'Start', 'End']] = temp.sj_id.str.split('_', expand=True)
        temp = temp[['Chromosome', 'Strand', 'Start', 'End', 'sj_id']].drop_duplicates()

        temp.Start = temp.Start.astype(int)
        temp.End = temp.End.astype(int)
        assert len(temp.loc[temp.Start>temp.End])==0

        # melt to 5' and 3'
        temp = temp.melt(id_vars=['Chromosome', 'Strand', 'sj_id'],
                         value_vars=['Start', 'End'])
        temp['sj_loc'] = ''
        temp.loc[temp.map_genome=='Start', 'sj_loc'] = 'start'
        temp.loc[temp.map_genome=='End', 'sj_loc'] = 'end'

        temp.rename({'value':'Start'}, axis=1, inplace=True)
        # need to verify that this is working using like one motif or something make
        # sure I don't have off-by-one errors
        # verified
        temp['Start'] = temp.Start-2
        temp['End'] = temp.Start+1

        # verified https://trello.com/c/fMhwX3s6
        temp.loc[temp.sj_loc=='start', 'Start'] = temp.loc[temp.sj_loc=='start', 'Start']-9
        temp.loc[temp.sj_loc=='start', 'End'] = temp.loc[temp.sj_loc=='start', 'End']+2


        temp.loc[temp.sj_loc=='end', 'End'] = temp.loc[temp.sj_loc=='end', 'End']+11
        temp.loc[temp.sj_loc=='end', 'Start'] = temp.loc[temp.sj_loc=='end', 'Start']

        temp.drop(['cell_line_id', 'map_genome'], axis=1, inplace=True)

        temp['len'] = temp.End-temp.Start
        assert len(temp.loc[temp.len!=12]) == 0

        temp = pr.PyRanges(temp)
        temp.to_bed(output.bed)

use rule bcftool_subset_on_samples_and_bed_intersect as sample_12bp_sj_ss_var with:
    input:
        vcf = config['1000g']['vcf'],
        bed = config['lr']['td_personal']['sqanti']['sj_12nt_bed']
    params:
        samples = lambda wc: wc.cell_line_id
    output:
        bed = config['lr']['td_personal']['exon_vars']['sj_12nt_vcf_intersect']

#
# rule bcftool_subset_on_samples_and_vcf_intersect:
#     resources:
#         threads = 8,
#         nodes = 2
#     conda:
#         'base'
#     shell:
#         """
#         module load bedtools
#         bcftools view \
#             --samples {params.samples} \
#             --threads {resources.threads} \
#             -Ov \
#             --min-ac=1 \
#             {input.vcf} | \
#         bedtools intersect \
#              -a stdin \
#              -b {input.bed} \
#              -wa \
#              -u > {output.vcf}
#         """
#
#
# # get the variants per sample that intersect w/ +-10bp exons or SS
# use rule bcftool_subset_on_samples_and_vcf_intersect as sample_12bp_sj_ss_var with:
#     input:
#         vcf = config['1000g']['vcf'],
#         bed = config['lr']['td_personal']['sqanti']['sj_12nt_bed']
#     params:
#         samples = lambda wc: wc.cell_line_id
#     output:
#         vcf = config['lr']['td_personal']['exon_vars']['sj_12nt_vcf_intersect_vcf']
