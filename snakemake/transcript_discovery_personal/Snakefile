import pandas as pd
import os
import sys
import pysam

p = os.path.dirname(os.path.dirname(os.getcwd()))+'/scripts/'
sys.path.append(p)

from sm_utils import *
from utils import *
# from vcf_utils import *

c_dir = '../common/'

meta_file = '../config.tsv'
configfile: '../config.yml'

include: f'{c_dir}download.smk'
include: f'{c_dir}samtools.smk'
include: f'{c_dir}winnowmap.smk'
include: f'{c_dir}bigwig.smk'
include: f'{c_dir}variant_calling.smk'
include: f'{c_dir}formatting.smk'
include: f'{c_dir}phasing.smk'
include: f'{c_dir}cerberus.smk'
include: f'{c_dir}bedtools.smk'
include: f'{c_dir}transdecoder.smk'
include: f'{c_dir}protein.smk'
include: f'{c_dir}minimap2.smk'
include: f'{c_dir}stats.smk'
include: f'{c_dir}lr-kallisto.smk'

# sample information
# sample information
df = load_meta()
df = df.loc[~df['sample'].str.contains('_')]
df['lab_sample'] = df['lab_number_sample'].astype(str)+'_'+\
                      df['lab_sampleid'].astype(str)+'_'+\
                      df['cell_line_id'].astype(str)
df = df[['cell_line_id', 'sample', 'hapmap_DNA_ID', 'lab_sample']].drop_duplicates()

temp_df = pd.read_csv('cell_line_ids.txt', header=None, names=['cell_line_id'])

# make a 1000g cell line id col
df['cell_line_id_1000g'] = df.cell_line_id

inds = df.loc[~df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())].index
df.loc[inds, 'cell_line_id_1000g'] = df.loc[inds, 'hapmap_DNA_ID']
len(df.index)

# limit to just those in 1000g
df = df.loc[df.cell_line_id_1000g.isin(temp_df.cell_line_id.tolist())]
assert len(df.index) == 30

# TODO bad sample that hasn't finished on espresso
bad_samples = ['NA19328']
df = df.loc[~df.cell_line_id_1000g.isin(bad_samples)]

hap = ['hap1', 'hap2']

wildcard_constraints:
    cell_line_id='|'.join([re.escape(x) for x in df['cell_line_id_1000g'].tolist()]),
    hap='|'.join([re.escape(x) for x in hap]),

rule all:
    input:
        config['lr']['td_personal']['sqanti']['sj_summary'],
        expand(config['lr']['espresso']['sqanti']['sjs'],
               cell_line_id=df['cell_line_id_1000g'].tolist()),
        expand(config['lr']['espresso']['cerb']['ics'],
               cell_line_id=df['cell_line_id_1000g'].tolist()),
        expand(config['lr']['td_personal']['sqanti']['sjs'],
               cell_line_id=df['cell_line_id_1000g'].tolist(),
               hap=hap),
        expand(config['lr']['td_personal']['cerb']['ics'],
               cell_line_id=df['cell_line_id_1000g'].tolist(),
               hap=hap),
        expand(config['lr']['td_personal']['map']['bam_ind'],
               cell_line_id=df['cell_line_id_1000g'].tolist(),
               hap=hap),

def get_df_val(df, col1, col_dict):
    temp = df.copy(deep=True)

    for key, item in col_dict.items():
        temp = temp.loc[temp[key] == item]

    val = temp[col1].unique()
    assert len(val) == 1
    return val[0]

### index genome
use rule minimap2_index as ind_alt_assembly with:
    input:
        fa = config['lr']['td_personal']['ref_fa']
    output:
        ind = config['lr']['td_personal']['ref_fa_mmi']

rule minimap2_with_index_2:
    resources:
        threads = 8,
        nodes = 32
    shell:
        """
        module load minimap2
        minimap2 \
            -ax splice \
            -t {resources.threads} \
            --MD \
            --secondary=no \
            -L \
            -o {output.sam} \
            -a {input.ind} \
            {input.fq}
        """

use rule minimap2_with_index_2 as alt_map with:
    input:
        fq = lambda wc: expand(config['lr']['fastq'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        ind = config['lr']['td_personal']['ref_fa_mmi']
    resources:
        threads = 8,
        nodes = 32
    output:
        sam = temporary(config['lr']['td_personal']['map']['sam'])
        # sam = temporary(config['lr']['td_personal']['map']['sam'])

use rule filt_non_prim_unmap_supp as alt_filt_map with:
    input:
        align = config['lr']['td_personal']['map']['sam']
    output:
        align = temporary(config['lr']['td_personal']['map']['sam_filt'])

use rule sam_to_bam as alt_sam_to_bam with:
    input:
        sam = config['lr']['td_personal']['map']['sam_filt']
    output:
        bam = temporary(config['lr']['td_personal']['map']['bam'])

# index and sort bam
use rule sort_bam as alt_sort_bam with:
    input:
        bam = config['lr']['td_personal']['map']['bam']
    output:
        bam = config['lr']['td_personal']['map']['bam_sort']

use rule index_bam as alt_index_bam with:
    input:
        bam = config['lr']['td_personal']['map']['bam_sort']
    output:
        ind = config['lr']['td_personal']['map']['bam_ind']

# ### statistics
# rule all_stats:
#     input:
#         config['lr']['fastq_n_reads_summary'],
#         expand(config['lr']['td_personal']['map']['bam_n_reads_summary'],
#                assembly=assemblies),
#         expand(config['lr']['td_personal']['map']['bam_mapqs_summary'],
#               assembly=assemblies)
#
# # raw fq read ids
# use rule fq_get_read_ids as fq_read_ids with:
#     input:
#         fq = lambda wc: expand(config['lr']['fastq'],
#                     lab_sample=get_df_val(df,
#                             'lab_rep',
#                             {'tech_rep': wc.sample}))
#     output:
#         txt = config['lr']['fastq_reads']
#
# # raw fq counts
# use rule fq_count_reads as count_raw_reads with:
#     input:
#         fq = lambda wc: expand(config['lr']['fastq'],
#                     lab_sample=get_df_val(df,
#                             'lab_rep',
#                             {'tech_rep': wc.sample}))
#     output:
#         txt = config['lr']['fastq_n_reads']
#
# use rule count_reads_summary as fq_count_reads_summary with:
#     input:
#         txts = lambda wc: expand(config['lr']['fastq_n_reads'],
#                                 sample=df['tech_rep'].tolist()),
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['fastq_n_reads_summary']
#
# # filtered bam counts
# use rule count_bam as alt_count_mapped_reads with:
#     input:
#         align = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         txt = config['lr']['td_personal']['map']['bam_n_reads']
#
#
# use rule count_reads_summary as sam_count_reads_summary with:
#     input:
#         txts = lambda wc: expand(config['lr']['td_personal']['map']['bam_n_reads'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist()),
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_n_reads_summary']
#
# # mapqs per read
# use rule bam_get_mapqs as alt_get_mapqs with:
#     input:
#         bam = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         txt = config['lr']['td_personal']['map']['bam_mapqs']
#
# use rule read_id_df_summary as alt_get_mapqs_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist())
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_mapqs_summary']
#
# # query coverage and read length per read
# use rule bam_get_query_cov as alt_get_qcov with:
#     input:
#         align = config['lr']['td_personal']['map']['bam_sort']
#     output:
#         out = config['lr']['td_personal']['map']['bam_query_covs']
#
# use rule read_id_df_summary as alt_get_qcov_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['bam_query_covs'],
#                                 assembly=wc.assembly,
#                                 sample=df['tech_rep'].tolist())
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['bam_query_covs_summary']
#
# ################################
# #### mapping summary statistics that aren't so heavy
# ################################
# use rule bool_mapq_summary as get_bool_mapq_summary with:
#     input:
#         files = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                               sample=wc.sample,
#                               assembly=assemblies)
#     params:
#         assemblies = assemblies,
#         mapq_thresh = lambda wc: wc.thresh
#     output:
#         tsv = config['lr']['td_personal']['map']['mapq_tsv'],
#         afr_reads = config['lr']['td_personal']['map']['mapq_uniq_afr_reads'],
#         upset = config['lr']['td_personal']['map']['mapq_upset']
#
# use rule max_mapq_summary as get_max_mapq_summary with:
#     input:
#         files = lambda wc: expand(config['lr']['td_personal']['map']['bam_mapqs'],
#                               sample=wc.sample,
#                               assembly=assemblies)
#     params:
#         assemblies = assemblies,
#         mapq_thresh = lambda wc: wc.thresh
#     output:
#         tsv = config['lr']['td_personal']['map']['max_mapq_tsv'],
#         afr_reads = config['lr']['td_personal']['map']['max_mapq_uniq_afr_reads'],
#         upset = config['lr']['td_personal']['map']['max_mapq_upset']
#
#
#
# use rule read_id_df_summary as get_mapq_summary_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['mapq_tsv'],
#                sample=df.tech_rep.tolist(),
#                thresh=wc.thresh)
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['mapq_tsv_summary']
#
# use rule read_id_df_summary as get_max_mapq_summary_summary with:
#     input:
#         tsvs = lambda wc: expand(config['lr']['td_personal']['map']['max_mapq_tsv'],
#                sample=df.tech_rep.tolist(),
#                thresh=wc.thresh)
#     resources:
#         threads = 1,
#         nodes = 3
#     params:
#         samples = df['tech_rep'].tolist()
#     output:
#         summ = config['lr']['td_personal']['map']['max_mapq_tsv_summary']


#############
##### sqanti
############


rule sqanti:
    resources:
        nodes = 2,
        threads = 8
    conda:
        'base'
    shell:
        """
        conda activate /gpfs/projects/bsc83/utils/conda_envs/SQANTI3-5.2.1
        mkdir -p {params.odir}
        python {params.sq_path}sqanti3_qc.py \
            {input.gtf} \
            {input.annot} \
            {input.fa} \
            -d {params.odir} \
            --report skip \
            --force_id_ignore \
            --aligner_choice minimap2 \
            --skipORF \
            -o {params.opref}
        """

use rule sqanti as sqanti_td with:
    input:
        gtf = config['lr']['td_personal']['espresso']['gtf'],
        fa = config['lr']['td_personal']['ref_fa'],
        annot = config['ref']['gtf']
    params:
        sq_path = config['software']['sqanti_path'],
        opref = config['lr']['td_personal']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
        odir = '/'.join(config['lr']['td_personal']['sqanti']['gff'].split('/')[:-1])+'/'
    output:
        gff = config['lr']['td_personal']['sqanti']['gff']

# get ics file
use rule gtf_to_ic as gc_gtf_to_ic with:
    input:
        gtf = config['lr']['td_personal']['espresso']['gtf']
    output:
        tsv = config['lr']['td_personal']['cerb']['ics']

##################
###### stuff for espresso w/ normal hg38

# get ics file
use rule gtf_to_ic as gc_gtf_to_ic_hg38 with:
    input:
        gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
    output:
        tsv = config['lr']['espresso']['cerb']['ics']

use rule sqanti as sqanti_td_hg38 with:
    input:
        gtf = lambda wc: expand(config['lr']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        fa = config['ref']['fa'],
        annot = config['ref']['gtf']
    params:
        sq_path = config['software']['sqanti_path'],
        opref = config['lr']['espresso']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
        odir = '/'.join(config['lr']['espresso']['sqanti']['gff'].split('/')[:-1])+'/'
    output:
        gff = config['lr']['espresso']['sqanti']['gff'],
        sjs = config['lr']['espresso']['sqanti']['sjs']

##################
###### stuff for espresso w/ personal genome haplotypes

# get ics file
use rule gtf_to_ic as gc_gtf_to_ic_td with:
    input:
        gtf = lambda wc: expand(config['lr']['td_personal']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
    output:
        tsv = config['lr']['td_personal']['cerb']['ics']

use rule sqanti as sqanti_td_td with:
    input:
        gtf = lambda wc: expand(config['lr']['td_personal']['espresso']['gtf'],
                       lab_sample=get_df_val(df,
                            'lab_sample',
                            {'cell_line_id_1000g': wc.cell_line_id})),
        fa = config['lr']['td_personal']['ref_fa'],
        annot = config['ref']['gtf']
    params:
        sq_path = config['software']['sqanti_path'],
        opref = config['lr']['td_personal']['sqanti']['gff'].split('/')[-1].split('_corrected')[0],
        odir = '/'.join(config['lr']['td_personal']['sqanti']['gff'].split('/')[:-1])+'/'
    output:
        gff = config['lr']['td_personal']['sqanti']['gff'],
        sjs = config['lr']['td_personal']['sqanti']['sjs']

####################
# summarize splice junctions

rule sj_summary:
    input:
        hg38_sjs = expand(config['lr']['espresso']['sqanti']['sjs'],
               cell_line_id=df['cell_line_id_1000g'].tolist()),
        td_hap1_sjs = expand(config['lr']['td_personal']['sqanti']['sjs'],
                   cell_line_id=df['cell_line_id_1000g'].tolist(),
                   hap='hap1'),
        td_hap2_sjs = expand(config['lr']['td_personal']['sqanti']['sjs'],
                  cell_line_id=df['cell_line_id_1000g'].tolist(),
                  hap='hap2'),
    resources:
        nodes = 1,
        threads = 1
    params:
        cell_line_id = df['cell_line_id_1000g'].tolist()
    output:
        sj_summary = config['lr']['td_personal']['sqanti']['sj_summary']
    run:
        hg38_sjs = list(input.hg38_sjs)
        td_hap1_sjs = list(input.td_hap1_sjs)
        td_hap2_sjs = list(input.td_hap2_sjs)

        genomes = ['hg38', 'hap1', 'hap2']

        df = pd.DataFrame()
        for genome, sjs in zip(genomes, [hg38_sjs, td_hap1_sjs, td_hap2_sjs]):
            for clid, sj_file in zip(param.cell_line_id, sjs):
                # quick check to make sure we're looking at the right
                # cell line for each file
                assert clid in sj_file
                temp = pd.read_csv(sj_file, sep='\t')
                temp['genome'] = genome
                temp['cell_line_id'] = clid
                df = pd.concat([df, temp], axis=0)
        df.to_csv(output.sj_summary, sep='\t')
